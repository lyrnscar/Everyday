监督学习，从输入、输出对中进行学习，每个样例输入对应一个预期输出，比如图像识别，信用检验
无监督学习，输入已知，没有为算法提供输出数据，确定主题，预先不知道分组
样本，行
特征，列
理解处理的数据与想解决任务的关系，理解数据集的内容
想回答的问题，已有数据能否回答，能否表达
用那种方法表示机器学习问题
提取数据哪些特征，能否实现正确预测
衡量成功，训练数据，测试数据，评估模型（计算精度）【knn的score】
分类，类别，标签，形状，泛化，过拟合（选择过于复杂的模型），欠拟合（选择过于简单的模型） 数据集大小，更复杂的模型
特征工程，包含导出特征
k近邻，1个邻居，离谁最近就选谁，k越多，模型越简单，精度下降
任意k个邻居（投票法）决策边界，模型易理解，预处理很重要，不适用都是0的稀疏数据集、很多特征的数据集
回归问题返回R^2决定系数
普通最小二乘法，ols均方误差最小，无参数，无法控制复杂度，训练集和测试集的性能差异是过拟合的明显标志
岭回归，正则化避免过拟合
如果数据足够多，正则化重要性下降，性能接近
模型性能作为数据集大小的函数进行绘图，学习曲线

lassoL1正则化是指权值向量www中各个元素的绝对值之和

岭回归L2正则化是指权值向量www中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号）

logistic回归，线性支持向量机两种线性分类方法
默认使用L2正则化，考虑更多特征时避免过拟合更重要

线性模型主要参数为正则化参数，回归模型中叫alpha，两个分类方法中叫c，alpha大c小说明较简单
特征少，可解释性强L1

贝叶斯，朴素、伯努利，训练快，泛化能力稍差，alpha平滑化，适用较大数据集，高维稀疏数据效果好，参数鲁棒性好（调整不重要）
决策树，节点一系列ifelse，深度，防止过拟合（泛化性能差），预剪枝，后剪枝，剪枝，查看特征重要性
随机森林，树的个数，性能好，解释性差，不适用维度高的稀疏数据
梯度提升决策树树（回归、分类）
svm核支持向量机，添加非线性特征，对样本个数缩放表现不好，预处理调参要求高
多层感知机，神经网络，加权求和，隐单元，训练时间长，均匀

最近邻
　　适用于小型数据集，是很好的基准模型，很容易解释。
线性模型
　　非常可靠的首选算法，适用于非常大的数据集，也适用于高维数据。
朴素贝叶斯
　　只适用于分类问题。比线性模型速度还快，适用于非常大的数据集和高维数据。精度通常要低于线性模型。
决策树
　　速度很快，不需要数据缩放，可以可视化，很容易解释。
随机森林
　　几乎总是比单棵决策树的表现要好，鲁棒性很好，非常强大。不需要数据缩放。不适用于高维稀疏数据。
梯度提升决策树
　　精度通常比随机森林略高。与随机森林相比，训练速度更慢，但预测速度更快，需要的内存也更少。比随机森林需要更多
的参数调节。
支持向量机
　　对于特征含义相似的中等大小的数据集很强大。需要数据缩放，对参数敏感。
神经网络
　　可以构建非常复杂的模型，特别是对于大型数据集而言。对数据缩放敏感，对参数选取敏感。大型网络需要很长的训练时
间。

评估无监督算法结果的唯一方法就是人工检查
无监督学习，缩放，主成分分析，高维旋转找到方向
kmeans聚类，找到代表数据特定区域的 簇中心
层次聚类 birch聚类特征树cf一边扫描
dbscan 核心样本，噪声
onehot编码表示分类变量，虚拟变量0/1特征

交叉验证，网格搜索
二分类指标，第一类，第二类错误，混淆矩阵，精度，准确率，召回率，f1分数
