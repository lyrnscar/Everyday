4.3 t-SNE
pca先旋转然后减少方向限制了有效性，正如面部识别的例子那样，有一类算法为流形学习算法，允许进行复杂映射，给出更好的可视化
流形学习主要用于可视化，很少用于生成两个以上新特征，t-SNE计算训练数据的一种新表示，不允许变换新数据
这些算法不能用于测试集，只能变换用于训练的数据，对探索性数据分析很有用（很少用于监督学习）
t-SNE的背后思想是找到数据的一个二维表示，尽可能保持数据点之间距离
首先给出每个数据点随机二维表示，然后尝试让在原始特征空间中距离较近的点更近，远的点更远，重点关注距离较近的点，而不是保持较远点间距离
即试图保存表示那些点比较靠近的信息
对手写数字数据集应用t-SNE
利用pca降维，对前两个主成分作图，按类别对数据点着色，发现利用前两个主成分可以较好的分开064
应用t-SNE，只能使用fit_transform方法，构建模型并立刻返回变换后的数据
效果非常好，这种方法不知道类别标签，完全无监督，但能找到二维表示，仅根据靠近程度就可将各个类别明确分开
参数包括perplexity和early_exaggeration作用一般很小

5聚类
将数据集划分成组的任务，这些组叫簇，目标是划分数据，为每个数据点分配或预测一个数字（标号）

5.1
k均值聚类
最简单最常用算法，试图找到数据特定区域簇中心
将每个数据点分配给最近的簇中心，然后将每个簇中心设置为所分配的所有数据点的平均值，如果簇的分配不再发生变化，那么算法结束
n_clusters默认值为8

过程与监督学习代码类似，先实例化，然后fit，即可在kmeans.labels_属性中找到标签
也可以用predict方法为新数据点分配簇标签，现有模型不改变，最近的簇中心分配给新数据点

标签不是真实的，标签本身没有先验意义
簇中心保存在cluster_centers_属性中

5.1.1
k均值失败案例
即使知道数据中簇的正确个数，该方法也可能不能总是找到它们
每个簇仅有其中心定义，意味着每个簇都是凸形的，k均值只能找到相对简单的形状，k均值还假设所有簇在某种程度上具有相同的“直径”，总是将簇之间的边界刚好画在簇中心的中心位置

簇的密度不同时，簇会包含一些远离簇中心的其他点
k均值还假设所有方向对每个簇同等重要，故无法识别非球形簇，如果形状更加复杂，那表现也不好
在例子中，k均值无法识别条形数据和半月形数据

5.1.2
矢量量化，或者将k均值看作分解
k均值和分解方法如pca.nmf有些相似，pca试图找到方差最大的方向，nmf试图找到累加分量通常对应数据的极值或部分，两种方法试图将数据点表示为一些分量之和
k均值尝试利用簇中心表示每个数据点，可以将其看作仅用一个分量表示每个数据点，该分量由簇中心给出，这种观点将k均值看作一种分解方法，每个点用单一分量表示，这种观点为矢量量化
比较三种方法，分别显示提取的分量，对k均值，重建就是在训练集中找到最近的簇中心
利用k均值可以用比输入维度更多的簇来对数据进行编码，通过使用更多的簇中心，可以用k均值找到一种更具表现力的表示
在两个半月的数据集上，利用10维表示，可以用线性模型划分两个半月，将到每个簇中心的距离作为特征还可以得到一种表现力更强的数据表示
利用transform方法完成这一点

k均值易于理解实现，非常流行速度快，可以扩展到大型数据集
缺点在于依赖随机初始化，默认情况下用10种不同的随机初始化将算法运行10次返回最佳结果
对簇形状的假设约束性较强，要求指定所要寻找的簇的个数（现实世界应用可能并不知道这个数）
